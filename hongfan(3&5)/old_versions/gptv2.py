import gradio as gr
import boto3
import uuid
import json
import openai
from datetime import datetime, timezone
import os
import asyncio
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è¿æ¥ AWS DynamoDB
session = boto3.Session(region_name=os.getenv("AWS_REGION"))
dynamodb_resource = session.resource("dynamodb")
table = dynamodb_resource.Table("chat_history")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ğŸ”¹ **1. æ•æ„Ÿä¿¡æ¯æ£€æµ‹æ¨¡å—**
# ğŸ”¹ **1. Improved Sensitive Info Detection**
async def detect_sensitive_info(user_message):
    """
    Calls OpenAI to classify user input sensitivity.
    Fixes JSON parsing errors by ensuring model outputs strict JSON format.
    """
    response = await asyncio.to_thread(
        client.chat.completions.create,
        model="gpt-3.5-turbo-1106",
        messages=[
            {"role": "system", "content": "Classify the user's message into 'not sensitive', 'sensitive', or 'very sensitive'. Output should be a JSON object with a single key 'sensitivity'."},
            {"role": "user", "content": user_message}
        ],
        temperature=0.3,
        response_format={"type": "json_object"}  # **Enforces valid JSON output**
    )

    try:
        classification = json.loads(response.choices[0].message.content.strip())  # Ensures clean parsing
        return classification.get("sensitivity", "not sensitive")  # Default fallback
    except json.JSONDecodeError as e:
        print(f"Error parsing sensitivity response: {e}")
        return "not sensitive"

# ğŸ”¹ **2. äº¤äº’é€»è¾‘ï¼šå¤„ç†æ•æ„Ÿä¿¡æ¯**
async def chatbot(user_id, session_id, user_message, chat_history):
    """
    ä¸»è¦å¯¹è¯é€»è¾‘ï¼š
    1. æ£€æµ‹ç”¨æˆ·è¾“å…¥æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯ã€‚
    2. å¦‚æœåŒ…å«ï¼Œå…ˆè¯¢é—®ç”¨æˆ·æ˜¯å¦åˆ é™¤è¯¥ä¿¡æ¯ã€‚
    3. ç”¨æˆ·é€‰æ‹©åˆ é™¤åˆ™ç”¨å ä½ç¬¦æ›¿æ¢ï¼Œå¦åˆ™æ­£å¸¸å¯¹è¯ã€‚
    """
    
    if not session_id:
        session_id = str(uuid.uuid4())
    if chat_history is None:
        chat_history = []

    # ğŸ”¸ **(1) æ£€æµ‹ç”¨æˆ·è¾“å…¥æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯**
    sensitivity = await detect_sensitive_info(user_message)

    if sensitivity in ["sensitive", "very sensitive"]:
        # æ’å…¥ä¸€æ¡ç³»ç»Ÿæ¶ˆæ¯ï¼Œæé†’ç”¨æˆ·è¾“å…¥äº†æ•æ„Ÿä¿¡æ¯ï¼Œå¹¶è¯¢é—®æ˜¯å¦åˆ é™¤
        chat_history.append({"role": "system", "content": f"You have input sensitive information ({sensitivity}). Do you want to remove it? (yes/no)"})
        
        # **ç­‰å¾…ç”¨æˆ·å›å¤æ˜¯å¦åˆ é™¤**
        return chat_history, user_id, session_id

    # ğŸ”¸ **(2) å¤„ç†ç”¨æˆ·å¯¹æ•æ„Ÿä¿¡æ¯çš„å›åº”**
    if chat_history and chat_history[-1]["role"] == "system" and "sensitive information" in chat_history[-1]["content"]:
        if user_message.lower() in ["yes", "remove", "delete"]:
            chat_history.append({"role": "system", "content": "User chose to remove sensitive information."})
        else:
            chat_history.append({"role": "system", "content": "User chose to keep the information."})

        return chat_history, user_id, session_id

    # ğŸ”¸ **(3) æ­£å¸¸å¯¹è¯æµç¨‹**
    messages = [{"role": "system", "content": "You are a helpful AI assistant."}] + chat_history
    messages.append({"role": "user", "content": user_message})

    # è°ƒç”¨ LLM ç”Ÿæˆå›å¤
    response = await asyncio.to_thread(
        client.chat.completions.create,
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0.7,
    )
    llm_response_text = response.choices[0].message.content

    # æ›´æ–°èŠå¤©è®°å½•
    chat_history.append({"role": "user", "content": user_message})
    chat_history.append({"role": "assistant", "content": llm_response_text})

    # å­˜å‚¨èŠå¤©è®°å½•
    await save_to_dynamodb(user_id, session_id, chat_history)

    return chat_history, user_id, session_id

# ğŸ”¹ **3. å­˜å‚¨èŠå¤©è®°å½•**
async def save_to_dynamodb(user_id, session_id, chat_history):
    data = {
        "user_id": user_id,
        "session_id": session_id,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "history": json.dumps(chat_history),
    }
    await asyncio.to_thread(table.put_item, Item=data)

# ğŸ”¹ **4. Gradio UI**
with gr.Blocks() as demo:
    gr.Markdown("# OpenAI LLM Chatbot with Private Message Highlighter")

    user_id_input = gr.Textbox(label="Enter Your User ID")
    session_id = gr.State("")
    chatbot_ui = gr.Chatbot(type="messages", label="Chatbot")
    user_input = gr.Textbox(label="Your Message")
    submit_button = gr.Button("Send")
    session_dropdown = gr.Dropdown(label="Select a Session", choices=[])

    # é€‰æ‹©ä¼šè¯ååŠ è½½èŠå¤©è®°å½•
    async def load_chat_history(user_id, session_id):
        if not session_id:
            return [], session_id
        response = await asyncio.to_thread(table.get_item, Key={"user_id": user_id, "session_id": session_id})
        return json.loads(response["Item"]["history"]), session_id if "Item" in response else [], session_id

    session_dropdown.change(load_chat_history, [user_id_input, session_dropdown], [chatbot_ui, session_id])

    # åˆ›å»ºæ–°ä¼šè¯
    def create_new_session(user_id):
        return str(uuid.uuid4()), []

    new_session_button = gr.Button("New Chat Session")
    new_session_button.click(create_new_session, [user_id_input], [session_id, chatbot_ui])

    # ç»‘å®šèŠå¤©åŠŸèƒ½
    submit_button.click(chatbot, [user_id_input, session_id, user_input, chatbot_ui], [chatbot_ui, user_id_input, session_id])

    # æ¸…ç©ºèŠå¤©
    clear_button = gr.Button("Clear Chat")
    clear_button.click(lambda: ([], ""), inputs=[], outputs=[chatbot_ui, session_id])

# å¯åŠ¨ Gradio
demo.launch(share=True)
